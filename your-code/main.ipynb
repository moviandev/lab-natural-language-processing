{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x6/sqc5t7mx3kq_pgqv290tb45w0000gn/T/ipykernel_9635/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_train.csv\", encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (800, 2)\n",
      "Validation set shape: (200, 2)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'label' is the target column based on the context of SPAM/HAM classification\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    data['text'],\n",
    "    data['label'],\n",
    "    test_size=0.2, # 80% train, 20% validation\n",
    "    random_state=42, \n",
    "    stratify=data['label'] # Important for balanced classes in NLP/classification\n",
    ")\n",
    "\n",
    "# Recombine into DataFrames for easier feature engineering later\n",
    "data_train = pd.DataFrame({'text': X_train, 'label': y_train}).reset_index(drop=True)\n",
    "data_val = pd.DataFrame({'text': X_val, 'label': y_val}).reset_index(drop=True)\n",
    "\n",
    "print(f\"Training set shape: {data_train.shape}\")\n",
    "print(f\"Validation set shape: {data_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  Dear=2C Good day hope fine=2Cdear am writting ...   \n",
      "1  FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...   \n",
      "2                                           Will do.   \n",
      "3  FROM THE DESK OF DR.ADAMU  ISMALERAUDITING AND...   \n",
      "4  Dear Friend, My name is LOI C.ESTRADA,The wife...   \n",
      "\n",
      "                                        text_cleaned  \n",
      "0  Dear=2C Good day hope fine=2Cdear am writting ...  \n",
      "1  FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...  \n",
      "2                                           Will do.  \n",
      "3  FROM THE DESK OF DR.ADAMU  ISMALERAUDITING AND...  \n",
      "4  Dear Friend, My name is LOI C.ESTRADA,The wife...  \n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "import re\n",
    "\n",
    "def clean_html(text):\n",
    "    # 1. Remove inline JavaScript/CSS (anything inside <script> and <style> tags)\n",
    "    text = re.sub(r'<script\\b[^>]*>.*?</script>', '', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    text = re.sub(r'<style\\b[^>]*>.*?</style>', '', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    # 2. Remove html comments\n",
    "    text = re.sub(r'', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # 3. Remove the remaining tags (e.g., <div>, <p>, <a>)\n",
    "    text = re.sub(r'<[^>]*>', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the cleaning to both datasets\n",
    "data_train['text_cleaned'] = data_train['text'].apply(clean_html)\n",
    "data_val['text_cleaned'] = data_val['text'].apply(clean_html)\n",
    "\n",
    "print(data_train[['text', 'text_cleaned']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        text_cleaned  \\\n",
      "0  Dear=2C Good day hope fine=2Cdear am writting ...   \n",
      "1  FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...   \n",
      "2                                           Will do.   \n",
      "3  FROM THE DESK OF DR.ADAMU  ISMALERAUDITING AND...   \n",
      "4  Dear Friend, My name is LOI C.ESTRADA,The wife...   \n",
      "\n",
      "                                   preprocessed_text  \n",
      "0  dear good day hope fine cdear am writting this...  \n",
      "1  from mr henry kaborethe chief auditor incharge...  \n",
      "2                                            will do  \n",
      "3  from the desk of dr adamu ismalerauditing and ...  \n",
      "4  dear friend my name is loi estrada the wife of...  \n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "def normalize_text(text):\n",
    "    # Convert to Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove prefixed 'b' (often seen in raw byte strings, though maybe not needed here)\n",
    "    text = re.sub(r\"^b\\\\'\", '', text)\n",
    "\n",
    "    # Remove all the special characters (keeping only letters and spaces)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    \n",
    "    # Substitute multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove all single characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)  \n",
    "\n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'^[a-zA-Z]\\s+', ' ', text) \n",
    "\n",
    "    return text\n",
    "\n",
    "from re import sub # Import re's sub function directly for brevity\n",
    "\n",
    "data_train['preprocessed_text'] = data_train['text_cleaned'].apply(normalize_text)\n",
    "data_val['preprocessed_text'] = data_val['text_cleaned'].apply(normalize_text)\n",
    "\n",
    "print(data_train[['text_cleaned', 'preprocessed_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    dear good day hope fine cdear writting mail du...\n",
      "1    mr henry kaborethe chief auditor inchargeforei...\n",
      "2                                                     \n",
      "3    desk dr adamu ismalerauditing accounting manag...\n",
      "4    dear friend name loi estrada wife mr josephest...\n",
      "Name: preprocessed_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "def remove_stopwords(text):\n",
    "    # Tokenize the text (split into words)\n",
    "    words = text.split()\n",
    "    \n",
    "    # Get the English stopwords list\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    \n",
    "    # Rejoin the words into a single string\n",
    "    return \" \".join(words)\n",
    "\n",
    "data_train['preprocessed_text'] = data_train['preprocessed_text'].apply(remove_stopwords)\n",
    "data_val['preprocessed_text'] = data_val['preprocessed_text'].apply(remove_stopwords)\n",
    "\n",
    "print(data_train['preprocessed_text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    dear good day hope fine cdear writ mail due re...\n",
      "1    mr henri kaboreth chief auditor inchargeforeig...\n",
      "2                                                     \n",
      "3    desk dr adamu ismaleraudit account manag bank ...\n",
      "4    dear friend name loi estrada wife mr josephest...\n",
      "Name: preprocessed_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "def stem_text(text):\n",
    "    # Tokenize the text\n",
    "    words = text.split()\n",
    "    \n",
    "    # Apply stemming\n",
    "    stemmed_words = [snowball.stem(w) for w in words]\n",
    "    \n",
    "    # Rejoin the words\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "data_train['preprocessed_text'] = data_train['preprocessed_text'].apply(stem_text)\n",
    "data_val['preprocessed_text'] = data_val['preprocessed_text'].apply(stem_text)\n",
    "\n",
    "print(data_train['preprocessed_text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top 10 Words in SPAM Messages ---\n",
      "money       756\n",
      "account     701\n",
      "bank        685\n",
      "fund        606\n",
      "us          597\n",
      "transact    437\n",
      "transfer    433\n",
      "busi        415\n",
      "countri     397\n",
      "foreign     394\n",
      "dtype: int64\n",
      "\n",
      "--- Top 10 Words in HAM Messages ---\n",
      "presid     97\n",
      "state      97\n",
      "work       97\n",
      "call       94\n",
      "would      92\n",
      "mr         85\n",
      "obama      82\n",
      "percent    80\n",
      "time       73\n",
      "one        69\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 1. Separate data by label\n",
    "spam_text = \" \".join(data_train[data_train['label'] == 1]['preprocessed_text'])\n",
    "ham_text = \" \".join(data_train[data_train['label'] == 0]['preprocessed_text'])\n",
    "\n",
    "# 2. Use CountVectorizer to get word counts\n",
    "vectorizer = CountVectorizer(stop_words=None, max_features=5000) # Simple counts\n",
    "\n",
    "# Fit on all text combined\n",
    "vectorizer.fit([spam_text, ham_text])\n",
    "\n",
    "# Transform and get feature names\n",
    "spam_counts = vectorizer.transform([spam_text])\n",
    "ham_counts = vectorizer.transform([ham_text])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 3. Get top 10 words for Spam\n",
    "spam_word_counts = pd.Series(spam_counts.toarray()[0], index=feature_names)\n",
    "top_10_spam = spam_word_counts.nlargest(10)\n",
    "print(\"--- Top 10 Words in SPAM Messages ---\")\n",
    "print(top_10_spam)\n",
    "\n",
    "# 4. Get top 10 words for Ham\n",
    "ham_word_counts = pd.Series(ham_counts.toarray()[0], index=feature_names)\n",
    "top_10_ham = ham_word_counts.nlargest(10)\n",
    "print(\"\\n--- Top 10 Words in HAM Messages ---\")\n",
    "print(top_10_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dear=2C Good day hope fine=2Cdear am writting ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear=2C Good day hope fine=2Cdear am writting ...</td>\n",
       "      <td>dear good day hope fine cdear writ mail due re...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...</td>\n",
       "      <td>1</td>\n",
       "      <td>FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...</td>\n",
       "      <td>mr henri kaboreth chief auditor inchargeforeig...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "      <td>Will do.</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FROM THE DESK OF DR.ADAMU  ISMALERAUDITING AND...</td>\n",
       "      <td>1</td>\n",
       "      <td>FROM THE DESK OF DR.ADAMU  ISMALERAUDITING AND...</td>\n",
       "      <td>desk dr adamu ismaleraudit account manag bank ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dear Friend, My name is LOI C.ESTRADA,The wife...</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Friend, My name is LOI C.ESTRADA,The wife...</td>\n",
       "      <td>dear friend name loi estrada wife mr josephest...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  Dear=2C Good day hope fine=2Cdear am writting ...      1   \n",
       "1  FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...      1   \n",
       "2                                           Will do.      0   \n",
       "3  FROM THE DESK OF DR.ADAMU  ISMALERAUDITING AND...      1   \n",
       "4  Dear Friend, My name is LOI C.ESTRADA,The wife...      1   \n",
       "\n",
       "                                        text_cleaned  \\\n",
       "0  Dear=2C Good day hope fine=2Cdear am writting ...   \n",
       "1  FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...   \n",
       "2                                           Will do.   \n",
       "3  FROM THE DESK OF DR.ADAMU  ISMALERAUDITING AND...   \n",
       "4  Dear Friend, My name is LOI C.ESTRADA,The wife...   \n",
       "\n",
       "                                   preprocessed_text  money_mark  \\\n",
       "0  dear good day hope fine cdear writ mail due re...           1   \n",
       "1  mr henri kaboreth chief auditor inchargeforeig...           0   \n",
       "2                                                              0   \n",
       "3  desk dr adamu ismaleraudit account manag bank ...           1   \n",
       "4  dear friend name loi estrada wife mr josephest...           1   \n",
       "\n",
       "   suspicious_words  text_len  \n",
       "0                 1       905  \n",
       "1                 1      1710  \n",
       "2                 0         0  \n",
       "3                 1       347  \n",
       "4                 1      1304  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"â‚¬\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CountVectorizer Shape ---\n",
      "Training BOW shape: (800, 26335)\n",
      "Validation BOW shape: (200, 26335)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_bow = count_vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "\n",
    "# Transform the validation data using the SAME fitted vectorizer\n",
    "X_val_bow = count_vectorizer.transform(data_val['preprocessed_text'])\n",
    "\n",
    "print(\"--- CountVectorizer Shape ---\")\n",
    "print(f\"Training BOW shape: {X_train_bow.shape}\")\n",
    "print(f\"Validation BOW shape: {X_val_bow.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TfidfVectorizer Shape ---\n",
      "Training TF-IDF shape: (800, 26335)\n",
      "Validation TF-IDF shape: (200, 26335)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "# TfidfVectorizer was already imported in Cell 3\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "\n",
    "# Transform the validation data using the SAME fitted vectorizer\n",
    "X_val_tfidf = tfidf_vectorizer.transform(data_val['preprocessed_text'])\n",
    "\n",
    "print(\"--- TfidfVectorizer Shape ---\")\n",
    "print(f\"Training TF-IDF shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Validation TF-IDF shape: {X_val_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MultinomialNB (TF-IDF) Performance ---\n",
      "Validation Accuracy: 0.9350\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.88      0.94       112\n",
      "           1       0.87      1.00      0.93        88\n",
      "\n",
      "    accuracy                           0.94       200\n",
      "   macro avg       0.94      0.94      0.93       200\n",
      "weighted avg       0.94      0.94      0.94       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Classifier must be MultinomialNB with default parameters\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Train the model using TF-IDF features\n",
    "nb_classifier.fit(X_train_tfidf, data_train['label'])\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = nb_classifier.predict(X_val_tfidf)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(data_val['label'], y_val_pred)\n",
    "\n",
    "print(\"--- MultinomialNB (TF-IDF) Performance ---\")\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(data_val['label'], y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Performance (TF-IDF + Extra Flags) ---\n",
      "Validation Accuracy (Combined Features): 0.6100\n",
      "\n",
      "Classification Report (Combined Features):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.30      0.47       112\n",
      "           1       0.53      1.00      0.69        88\n",
      "\n",
      "    accuracy                           0.61       200\n",
      "   macro avg       0.77      0.65      0.58       200\n",
      "weighted avg       0.79      0.61      0.57       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- Option 4: TF-IDF + Extra Flags (The best combination for the extra task) ---\n",
    "\n",
    "# 1. Vectorize text (already done in Cell 25, just reuse the matrices)\n",
    "# X_train_tfidf, X_val_tfidf\n",
    "\n",
    "# 2. Extract extra features\n",
    "# Note: Extra features were created in Cell 21. We access them here.\n",
    "extra_features_train = data_train[['money_mark', 'suspicious_words', 'text_len']]\n",
    "extra_features_val = data_val[['money_mark', 'suspicious_words', 'text_len']]\n",
    "\n",
    "# 3. Horizontally stack the TF-IDF matrix with the extra features\n",
    "X_train_combined = hstack([X_train_tfidf, extra_features_train.values])\n",
    "X_val_combined = hstack([X_val_tfidf, extra_features_val.values])\n",
    "\n",
    "# 4. Train the Multinomial Naive Bayes Classifier\n",
    "nb_classifier_combined = MultinomialNB()\n",
    "nb_classifier_combined.fit(X_train_combined, data_train['label'])\n",
    "\n",
    "# 5. Predict and Evaluate\n",
    "y_val_pred_combined = nb_classifier_combined.predict(X_val_combined)\n",
    "accuracy_combined = accuracy_score(data_val['label'], y_val_pred_combined)\n",
    "\n",
    "print(\"--- Performance (TF-IDF + Extra Flags) ---\")\n",
    "print(f\"Validation Accuracy (Combined Features): {accuracy_combined:.4f}\")\n",
    "print(\"\\nClassification Report (Combined Features):\")\n",
    "print(classification_report(data_val['label'], y_val_pred_combined))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
